{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mRs770o5H1tN"
   },
   "source": [
    "# 1 - Simple Question Classification\n",
    "\n",
    "In this series, we'll develop a deep learning model capable of classifying questions into predefined categories using PyTorch and TorchText. This task will be performed on the [TREC question dataset](https://cogcomp.seas.upenn.edu/Data/QA/QC/), a collection of questions designed to be classified into a small set of categories.\n",
    "\n",
    "In this initial notebook, we aim to grasp the fundamental concepts without a primary focus on achieving high accuracy. Subsequent notebooks will expand upon this foundation to improve the model's performance.\n",
    "\n",
    "### Introduction\n",
    "\n",
    "For analyzing sequences such as sentences in questions, we'll utilize a **recurrent neural network** (RNN). RNNs are particularly suited for sequence analysis, as they process sequences of words, $X=\\{x_1, ..., x_T\\}$, one at a time. Each word in the sequence is associated with a _hidden state_, $h$, which is updated sequentially. The RNN operates _recurrently_ by taking in the current word $x_t$ and the hidden state from the preceding word, $h_{t-1}$, to generate the next hidden state, $h_t$.\n",
    "\n",
    "$$h_t = \\text{RNN}(x_t, h_{t-1})$$\n",
    "\n",
    "Upon processing the last word in the question, $x_T$, and obtaining the final hidden state, $h_T$, this state is passed through a linear layer, $f$, to predict the question's category, $\\hat{y} = f(h_T)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q7PlOzNmH1tP"
   },
   "source": [
    "## Preparing Data\n",
    "\n",
    "For processing textual data in PyTorch, TorchText provides a powerful abstraction through the concept of `Field`. These define how to process your data, which, in the context of the TREC question classification task, includes the raw text of the question and its category label.\n",
    "\n",
    "### Fields Definition\n",
    "\n",
    "To process the questions and their labels, we define two fields: `TEXT` for the questions and `LABEL` for the categories.\n",
    "\n",
    "- The `TEXT` field specifies how the questions should be processed. We employ `tokenize='spacy'` as an argument to use the [spaCy](https://spacy.io) tokenizer, which is more sophisticated than simple space-based tokenization. This requires specifying `tokenizer_language` to indicate the spaCy model to use, typically `en_core_web_sm` for English text. This model must be downloaded prior to running your notebook using the command: `python -m spacy download en_core_web_sm`.\n",
    "\n",
    "- The `LABEL` field is managed by a `LabelField`, a variant of the `Field` designed for handling categorical data. The `dtype` argument will be clarified subsequently, ensuring the labels are processed appropriately for classification.\n",
    "\n",
    "For a deeper dive into `Fields` and their configuration, refer to [this Documentation](https://torchtext.readthedocs.io/en/latest/data.html#field).\n",
    "\n",
    "### Random Seed for Reproducibility\n",
    "\n",
    "To ensure that our experiments can be replicated, we set the random seed at the beginning of our preprocessing script. This controls the randomness involved in splitting the dataset and initializing the model weights, making our results reproducible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "hVuNibrCUAdf"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (3.7.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spacy) (8.2.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spacy) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spacy) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spacy) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spacy) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spacy) (4.66.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spacy) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spacy) (2.6.4)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spacy) (3.1.3)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spacy) (65.5.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spacy) (24.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spacy) (1.26.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.16.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.2.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jinja2->spacy) (2.1.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from en-core-web-sm==3.7.1) (3.7.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.28.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.6.4)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (65.5.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (23.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.24.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.16.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2022.12.7)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.2)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6309,
     "status": "ok",
     "timestamp": 1708247161452,
     "user": {
      "displayName": "Wenya Wang",
      "userId": "11693014802286625875"
     },
     "user_tz": -480
    },
    "id": "cD11k9FQIxzW",
    "outputId": "0d472058-3e02-4066-9e6d-4623c5b4dfc0",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchtext==0.4.0\n",
      "  Using cached torchtext-0.4.0-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torchtext==0.4.0) (4.66.1)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torchtext==0.4.0) (2.28.2)\n",
      "Requirement already satisfied: torch in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torchtext==0.4.0) (2.0.1)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torchtext==0.4.0) (1.24.3)\n",
      "Requirement already satisfied: six in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torchtext==0.4.0) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->torchtext==0.4.0) (3.0.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->torchtext==0.4.0) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->torchtext==0.4.0) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->torchtext==0.4.0) (2022.12.7)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch->torchtext==0.4.0) (3.11.0)\n",
      "Requirement already satisfied: typing-extensions in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch->torchtext==0.4.0) (4.10.0)\n",
      "Requirement already satisfied: sympy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch->torchtext==0.4.0) (1.11.1)\n",
      "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch->torchtext==0.4.0) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch->torchtext==0.4.0) (3.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from jinja2->torch->torchtext==0.4.0) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sympy->torch->torchtext==0.4.0) (1.3.0)\n",
      "Using cached torchtext-0.4.0-py3-none-any.whl (53 kB)\n",
      "Installing collected packages: torchtext\n",
      "Successfully installed torchtext-0.4.0\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m pip install  torchtext==0.4.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "diXAJf2jH1tQ"
   },
   "outputs": [],
   "source": [
    "from torchtext import data\n",
    "import torch\n",
    "\n",
    "# For tokenization\n",
    "TEXT = data.Field(tokenize='spacy',\n",
    "                  tokenizer_language='en_core_web_sm',\n",
    "                  include_lengths=True)\n",
    "\n",
    "# For multi-class classification labels\n",
    "LABEL = data.LabelField()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nX88mkuKH1tQ"
   },
   "source": [
    "Another convenient aspect of TorchText is its built-in support for widely recognized datasets in natural language processing (NLP).\n",
    "\n",
    "The code snippet below will automatically download the TREC question classification dataset and split it into standard train/test divisions as `torchtext.datasets` objects. It processes the data using the `Fields` we previously defined. The TREC dataset consists of thousands of questions that are categorized into both coarse-grained and fine-grained labels indicating the type of question (e.g., location, person, numeric information).\n",
    "\n",
    "\n",
    "\n",
    "**Note:** In this example, `fine_grained=False` specifies that we want to use the coarse label set. If you're interested in a more detailed classification, you can set `fine_grained=True` to use the fine-grained label set instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "nOTiebiKH1tR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading train_5500.label\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_5500.label: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 336k/336k [00:01<00:00, 302kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading TREC_10.label\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TREC_10.label: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 23.4k/23.4k [00:00<00:00, 76.2kB/s]\n"
     ]
    }
   ],
   "source": [
    "from torchtext import datasets\n",
    "\n",
    "# Load the TREC dataset\n",
    "train_data, test_data = datasets.TREC.splits(TEXT, LABEL, fine_grained=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "afgL6u_GH1tR"
   },
   "source": [
    "We can see how many examples are in each split by checking their length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 379,
     "status": "ok",
     "timestamp": 1708247177163,
     "user": {
      "displayName": "Wenya Wang",
      "userId": "11693014802286625875"
     },
     "user_tz": -480
    },
    "id": "ZRJ49dUVH1tR",
    "outputId": "a76b9361-f4d5-4c76-a221-e009deba7700"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 5452\n",
      "Number of testing examples: 500\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of training examples: {len(train_data)}')\n",
    "print(f'Number of testing examples: {len(test_data)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WOZlrCq0H1tR"
   },
   "source": [
    "We can also check an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 409,
     "status": "ok",
     "timestamp": 1708247180234,
     "user": {
      "displayName": "Wenya Wang",
      "userId": "11693014802286625875"
     },
     "user_tz": -480
    },
    "id": "a_GxVB6iH1tR",
    "outputId": "5290acdc-57f7-4cef-d41b-79135b263e72"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['How', 'did', 'serfdom', 'develop', 'in', 'and', 'then', 'leave', 'Russia', '?'], 'label': 'DESC'}\n"
     ]
    }
   ],
   "source": [
    "print(vars(train_data.examples[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cn0-tCRqH1tS"
   },
   "source": [
    "The TREC dataset, like the IMDb dataset, comes primarily with train and test splits, necessitating the creation of a validation set for a comprehensive evaluation process. We can achieve this through the use of the `.split()` method on the training data.\n",
    "\n",
    "While the default behavior of `.split()` divides the data into a 70/30 split, this can be customized with the `split_ratio` parameter. For example, setting `split_ratio` to 0.8 indicates that 80% of the data will be used for training, while the remaining 20% forms the validation set.\n",
    "\n",
    "To ensure consistency in our splits across different runs, we'll use the `random_state` parameter, passing in our predefined random seed. This guarantees that we receive the same train/validation split each time we execute our code.\n",
    "\n",
    "Here is how you can apply this to the TREC dataset:\n",
    "\n",
    "\n",
    "\n",
    "This approach allows us to maintain a structured and effective training regime, ensuring that our model is not only trained but also validated on a distinct subset of the dataset before being evaluated on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "eCXoOIyVH1tS"
   },
   "outputs": [],
   "source": [
    "from torchtext.data import Dataset\n",
    "\n",
    "# Assuming train_data is already defined\n",
    "train_data, valid_data = train_data.split(split_ratio=0.8, random_state=random.seed(SEED))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l7GIAq-lH1tS"
   },
   "source": [
    "Again, we'll view how many examples are in each split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 307,
     "status": "ok",
     "timestamp": 1708247186370,
     "user": {
      "displayName": "Wenya Wang",
      "userId": "11693014802286625875"
     },
     "user_tz": -480
    },
    "id": "aO3xIgKBH1tS",
    "outputId": "4adc259d-cbe3-4f55-fa51-2c2346ef2949"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 4362\n",
      "Number of validation examples: 1090\n",
      "Number of testing examples: 500\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of training examples: {len(train_data)}')\n",
    "print(f'Number of validation examples: {len(valid_data)}')\n",
    "print(f'Number of testing examples: {len(test_data)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wjmk1g3YH1tS"
   },
   "source": [
    "Next, we proceed to construct a _vocabulary_ for our dataset. This step is critical as machine learning models don't process raw text but numbers. The vocabulary serves as a lookup table where every unique word across the dataset is associated with a unique _index_ (an integer).\n",
    "\n",
    "In practice, each word's _index_ is utilized to generate a _one-hot_ vector, which is a sparse vector where all elements are 0, except for the position corresponding to the word's index, which is set to 1. The total number of unique words in the dataset defines the dimensionality of these one-hot vectors, often represented by $V$.\n",
    "\n",
    "Given the potentially vast size of our dataset's vocabulary, direct utilization of one-hot vectors can lead to inefficiencies in training and model size, especially considering some words might be very rare or unique to specific examples.\n",
    "\n",
    "To mitigate this, we can limit our vocabulary in two ways: either by retaining only the top $n$ most frequent words or by excluding words that appear fewer than $m$ times within the dataset. For the TREC task, we'll adopt the former strategy, focusing on the most common words to ensure our model's manageability and performance.\n",
    "\n",
    "Words not included in our reduced vocabulary are replaced with a special _unknown_ (`<unk>`) token. This approach ensures that our model can handle any word, even those not explicitly learned during training. For instance, if we encounter a sentence like \"What is the capital of France?\" and \"capital\" was not among the retained vocabulary words, it would be represented as \"What is the `<unk>` of France?\" in our processed data.\n",
    "\n",
    "Here's how we can build the vocabulary for the TREC dataset, specifying a `max_size` to limit its scope:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "Kzq6r-FtH1tS"
   },
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train_data, max_size=10000)\n",
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "afFTzzi7H1tS"
   },
   "source": [
    "Why do we only build the vocabulary on the training set? When testing any machine learning system you do not want to look at the test set in any way. We do not include the validation set as we want it to reflect the test set as much as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 311,
     "status": "ok",
     "timestamp": 1708247192524,
     "user": {
      "displayName": "Wenya Wang",
      "userId": "11693014802286625875"
     },
     "user_tz": -480
    },
    "id": "j8GmgGjAH1tS",
    "outputId": "19f11e5b-4476-49dd-88ef-fb1880834639"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in TEXT vocabulary: 8106\n",
      "Unique tokens in LABEL vocabulary: 6\n"
     ]
    }
   ],
   "source": [
    "print(f\"Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}\")\n",
    "print(f\"Unique tokens in LABEL vocabulary: {len(LABEL.vocab)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fIKDMaiSXxTp"
   },
   "source": [
    "We can also see the vocabulary directly using either the `stoi` (**s**tring **to** **i**nt) or `itos` (**i**nt **to**  **s**tring) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 324,
     "status": "ok",
     "timestamp": 1708247195493,
     "user": {
      "displayName": "Wenya Wang",
      "userId": "11693014802286625875"
     },
     "user_tz": -480
    },
    "id": "3U8jb9WWXyq-",
    "outputId": "863036e5-2ba6-4b6f-d8ce-b64531b45e02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<unk>\n",
      "<pad>\n",
      "?\n",
      "the\n",
      "What\n",
      "is\n",
      "of\n",
      "in\n",
      "a\n",
      "`\n"
     ]
    }
   ],
   "source": [
    "# Assuming TEXT.vocab has been built\n",
    "for i in range(10):\n",
    "    print(TEXT.vocab.itos[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B-8Giye0H1tS"
   },
   "source": [
    "\n",
    "Note there are two additional tokens: the `<unk>` token and the `<pad>` token.\n",
    "\n",
    "When we feed sentences into our model, we feed a _batch_ of them at a time, i.e. more than one at a time, and all sentences in the batch need to be the same size. Thus, to ensure each sentence in the batch is the same size, any shorter than the longest within the batch are padded.\n",
    "\n",
    "We can also view the most common words in the vocabulary and their frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 433,
     "status": "ok",
     "timestamp": 1708247199108,
     "user": {
      "displayName": "Wenya Wang",
      "userId": "11693014802286625875"
     },
     "user_tz": -480
    },
    "id": "Cy0-pS3oH1tT",
    "outputId": "81acf730-f702-43eb-a81a-40d85d6a82b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('?', 4281), ('the', 2875), ('What', 2594), ('is', 1347), ('of', 1221), ('in', 894), ('a', 797), ('`', 663), ('How', 598), (\"'s\", 571), ('was', 510), ('to', 493), (',', 454), ('Who', 452), ('for', 370), ('are', 349), ('and', 348), (\"''\", 319), ('does', 318), ('did', 304)]\n"
     ]
    }
   ],
   "source": [
    "print(TEXT.vocab.freqs.most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V3n_Iv61H1tT"
   },
   "source": [
    "We can also check the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 346,
     "status": "ok",
     "timestamp": 1708247202352,
     "user": {
      "displayName": "Wenya Wang",
      "userId": "11693014802286625875"
     },
     "user_tz": -480
    },
    "id": "JYFR21-NH1tT",
    "outputId": "42dd5671-2a5e-445e-b137-c71ef5573d52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(None, {'HUM': 0, 'ENTY': 1, 'DESC': 2, 'NUM': 3, 'LOC': 4, 'ABBR': 5})\n"
     ]
    }
   ],
   "source": [
    "print(LABEL.vocab.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 318,
     "status": "ok",
     "timestamp": 1708247205062,
     "user": {
      "displayName": "Wenya Wang",
      "userId": "11693014802286625875"
     },
     "user_tz": -480
    },
    "id": "-AC55M4-TkT-",
    "outputId": "f01f0aa2-cc85-4206-accf-37b6ee8f6a6d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequential': True,\n",
       " 'use_vocab': True,\n",
       " 'init_token': None,\n",
       " 'eos_token': None,\n",
       " 'unk_token': '<unk>',\n",
       " 'fix_length': None,\n",
       " 'dtype': torch.int64,\n",
       " 'preprocessing': None,\n",
       " 'postprocessing': None,\n",
       " 'lower': False,\n",
       " 'tokenizer_args': ('spacy', 'en_core_web_sm'),\n",
       " 'tokenize': functools.partial(<function _spacy_tokenize at 0x2891e19e0>, spacy=<spacy.lang.en.English object at 0x287f8d550>),\n",
       " 'include_lengths': True,\n",
       " 'batch_first': False,\n",
       " 'pad_token': '<pad>',\n",
       " 'pad_first': False,\n",
       " 'truncate_first': False,\n",
       " 'stop_words': None,\n",
       " 'is_target': False,\n",
       " 'vocab': <torchtext.vocab.Vocab at 0x28c8d65d0>}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zG_SfWp3H1tT"
   },
   "source": [
    "The final step of preparing the data is creating the iterators. We iterate over these in the training/evaluation loop, and they return a batch of examples (indexed and converted into tensors) at each iteration.\n",
    "\n",
    "We'll use a `BucketIterator` which is a special type of iterator that will return a batch of examples where each example is of a similar length, minimizing the amount of padding per example.\n",
    "\n",
    "We also want to place the tensors returned by the iterator on the GPU (if you're using one). PyTorch handles this using `torch.device`, we then pass this device to the iterator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5QYGHCMfH1tT"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data),\n",
    "    batch_size = BATCH_SIZE,\n",
    "    sort_within_batch = True,\n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XqygCp2RH1tT"
   },
   "source": [
    "## Build the Model\n",
    "\n",
    "As we progress, our next objective is to construct the model that will be trained and evaluated on the TREC question classification task.\n",
    "\n",
    "Creating models in PyTorch involves a bit of standardized code. Observe how our `RNN` class extends `nn.Module`, leveraging `super` to initialize it properly.\n",
    "\n",
    "Within the `__init__` method, we establish the model's _layers_. For our task, these include an _embedding_ layer, the RNN itself, and a _linear_ layer to output predictions. By default, these layers have randomly initialized parameters.\n",
    "\n",
    "The _embedding_ layer transforms sparse one-hot vectors (mostly zeros) into dense embedding vectors. This layer acts as a fully connected network that not only reduces input dimensionality but also groups semantically similar words closer in the embedding space. Embeddings are crucial for capturing the nuances of language in a computationally efficient manner.\n",
    "\n",
    "Following is the RNN layer, which processes the dense vector alongside the previous hidden state, $h_{t-1}$, to compute the subsequent hidden state, $h_t$. This recurrent processing allows the model to maintain a memory of past words as it moves through a sentence, essential for understanding the context and classifying questions accurately.\n",
    "\n",
    "The final step in our model involves the linear layer, which takes the RNN's last hidden state and projects it onto the space of our label categories, effectively determining the question's classification.\n",
    "\n",
    "During the `forward` method execution, we feed batches into our model. Each batch, `text`, is a tensor of dimensions _**[sentence length, batch size]**_, representing a batch of sentences where words are indicated by their indices.\n",
    "\n",
    "Though one might expect an additional dimension for one-hot vectors, PyTorch efficiently uses the index values directly, a process known as *numericalizing*.\n",
    "\n",
    "After passing through the embedding layer to obtain `embedded` (dimension _**[sentence length, batch size, embedding dim]**_), this tensor is input into the RNN. PyTorch's RNN can automatically start with an all-zero initial hidden state if none is provided.\n",
    "\n",
    "The RNN outputs two tensors: `output`, with size _**[sentence length, batch size, hidden dim]**_, representing the hidden state across all steps, and `hidden`, size _**[1, batch size, hidden dim]**_, the final hidden state. We use `hidden` to confirm our understanding, applying `squeeze` to remove any singleton dimensions.\n",
    "\n",
    "The prediction is generated by passing `hidden` through the linear layer, `fc`, aligning it with our classification goals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q5IWVRASH1tT"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, text, text_lengths):\n",
    "        #text = [sent len, batch size]\n",
    "\n",
    "        embedded = self.embedding(text)\n",
    "        #embedded = [sent len, batch size, emb dim]\n",
    "\n",
    "        output, hidden = self.rnn(embedded)\n",
    "        #output = [sent len, batch size, hid dim]\n",
    "        #hidden = [1, batch size, hid dim]\n",
    "\n",
    "        #assert torch.equal(output[-1,:,:], hidden.squeeze(0))\n",
    "\n",
    "        return self.fc(hidden.squeeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Grztahq0H1tT"
   },
   "source": [
    "We now create an instance of our RNN class.\n",
    "\n",
    "The input dimension is the dimension of the one-hot vectors, which is equal to the vocabulary size.\n",
    "\n",
    "The embedding dimension is the size of the dense word vectors. This is usually around 50-250 dimensions, but depends on the size of the vocabulary.\n",
    "\n",
    "The hidden dimension is the size of the hidden states. This is usually around 100-500 dimensions, but also depends on factors such as on the vocabulary size, the size of the dense vectors and the complexity of the task.\n",
    "\n",
    "The output dimension is usually the number of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C-NMrSMCH1tT"
   },
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 50\n",
    "OUTPUT_DIM = len(LABEL.vocab)\n",
    "\n",
    "model = RNN(VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "epWFTT7gH1tU"
   },
   "source": [
    "Let's also create a function that will tell us how many trainable parameters our model has so we can compare the number of parameters across different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 403,
     "status": "ok",
     "timestamp": 1708249354757,
     "user": {
      "displayName": "Wenya Wang",
      "userId": "11693014802286625875"
     },
     "user_tz": -480
    },
    "id": "wcSM1EMeH1tU",
    "outputId": "7b264e87-40c4-43e3-d953-4388a80a99a0"
   },
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wrkr-KlaH1tU"
   },
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ATDO1XbH1tU"
   },
   "source": [
    "Now, we'll proceed to configure the training phase and then embark on training our model.\n",
    "\n",
    "Initially, we select an optimizer, a crucial component in the training process. The optimizer is responsible for updating the model's parameters in response to the computed gradients during training. For our purposes, we'll employ _stochastic gradient descent_ (SGD) as our optimization algorithm. SGD is a popular choice due to its simplicity and effectiveness in various tasks, including NLP. The optimizer requires two primary inputs: the parameters to update and the learning rate, which determines the magnitude of the updates during the training process.\n",
    "\n",
    "Here's how you can set up SGD for our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GvMIccglH1tU"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Assume model is an instance of our RNN class\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w2VjOUWDH1tU"
   },
   "source": [
    "Next, we'll define our loss function, often referred to as a criterion in PyTorch terminology.\n",
    "\n",
    "For the TREC question classification task, which involves categorizing questions into multiple categories, the appropriate loss function is _cross-entropy loss_. This loss function is well-suited for multi-class classification problems.\n",
    "\n",
    "Unlike the binary case, where the model outputs a single scalar representing the probability of the positive class, in multi-class classification, the model outputs a vector of scores for each class. We aim to have the score for the correct class to be higher than those for all other classes.\n",
    "\n",
    "PyTorch provides `nn.CrossEntropyLoss` for this purpose, which combines a `softmax` layer and the cross-entropy loss in a single class. This means it first applies the softmax function to the model's outputs (to obtain probabilities across classes) and then computes the cross-entropy loss between these probabilities and the target distribution.\n",
    "\n",
    "Here's how you can define the criterion for the TREC task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y24HzSVUH1tU"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hrk3GksVeqwm"
   },
   "source": [
    "It's important to note that when using `nn.CrossEntropyLoss`, the target labels for your training data should be encoded as class indices (e.g., `0` for the first class, `1` for the second class, etc.), rather than one-hot vectors. Additionally, the input to this loss function (i.e., the model's output) should not have the softmax function applied; `nn.CrossEntropyLoss` expects raw scores because it internally applies the softmax."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QLggV-msH1tU"
   },
   "source": [
    "Using `.to`, we can place the model and the criterion on the GPU (if we have one)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oxpv2NL9H1tU"
   },
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DLgOX9sVH1tU"
   },
   "source": [
    "The `train` function iterates over all examples, one batch at a time.\n",
    "\n",
    "`model.train()` is used to put the model in \"training mode\", which turns on _dropout_ and _batch normalization_. Although we aren't using them in this model, it's good practice to include it.\n",
    "\n",
    "For each batch, we first zero the gradients. Each parameter in a model has a `grad` attribute which stores the gradient calculated by the `criterion`. PyTorch does not automatically remove (or \"zero\") the gradients calculated from the last gradient calculation, so they must be manually zeroed.\n",
    "\n",
    "We then feed the batch of sentences, `batch.text`, into the model. Note, you do not need to do `model.forward(batch.text)`, simply calling the model works. The `squeeze` is needed as the predictions are initially size _**[batch size, 1]**_, and we need to remove the dimension of size 1 as PyTorch expects the predictions input to our criterion function to be of size _**[batch size]**_.\n",
    "\n",
    "The loss and accuracy are then calculated using our predictions and the labels, `batch.label`, with the loss being averaged over all examples in the batch.\n",
    "\n",
    "We calculate the gradient of each parameter with `loss.backward()`, and then update the parameters using the gradients and optimizer algorithm with `optimizer.step()`.\n",
    "\n",
    "The loss and accuracy is accumulated across the epoch, the `.item()` method is used to extract a scalar from a tensor which only contains a single value.\n",
    "\n",
    "Finally, we return the loss and accuracy, averaged across the epoch. The `len` of an iterator is the number of batches in the iterator.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nqCbCqPuH1tU"
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    total_correct = 0\n",
    "    total_instances = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for batch in iterator:\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        text, text_lengths = batch.text\n",
    "\n",
    "        # Assuming your model's forward method automatically handles padding, then no need to pack sequence here\n",
    "        predictions = model(text, text_lengths)\n",
    "\n",
    "        loss = criterion(predictions, batch.label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # Compute the number of correct predictions\n",
    "        _, predicted_classes = predictions.max(dim=1)\n",
    "        correct_predictions = (predicted_classes == batch.label).float()  # Convert to float for summation\n",
    "        total_correct += correct_predictions.sum().item()\n",
    "        total_instances += batch.label.size(0)\n",
    "\n",
    "    epoch_acc = total_correct / total_instances\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mjgL4FpwH1tU"
   },
   "source": [
    "`evaluate` is similar to `train`, with a few modifications as you don't want to update the parameters when evaluating.\n",
    "\n",
    "`model.eval()` puts the model in \"evaluation mode\", this turns off _dropout_ and _batch normalization_. Again, we are not using them in this model, but it is good practice to include them.\n",
    "\n",
    "No gradients are calculated on PyTorch operations inside the `with no_grad()` block. This causes less memory to be used and speeds up computation.\n",
    "\n",
    "The rest of the function is the same as `train`, with the removal of `optimizer.zero_grad()`, `loss.backward()` and `optimizer.step()`, as we do not update the model's parameters when evaluating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eS3rZWzhH1tU"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    total_correct = 0\n",
    "    total_instances = 0\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for batch in iterator:\n",
    "            text, text_lengths = batch.text\n",
    "\n",
    "            # Assuming your model's forward method automatically handles padding, then no need to pack sequence here\n",
    "            predictions = model(text, text_lengths)\n",
    "\n",
    "            loss = criterion(predictions, batch.label)\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            # Compute the number of correct predictions\n",
    "            _, predicted_classes = predictions.max(dim=1)\n",
    "            correct_predictions = (predicted_classes == batch.label).float()  # Convert to float for summation\n",
    "            total_correct += correct_predictions.sum().item()\n",
    "            total_instances += batch.label.size(0)\n",
    "\n",
    "    epoch_acc = total_correct / total_instances\n",
    "    return epoch_loss / len(iterator), epoch_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ihiNgpLMH1tU"
   },
   "source": [
    "We'll also create a function to tell us how long an epoch takes to compare training times between models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EzkEdOh_H1tU"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M5qBxiXfH1tU"
   },
   "source": [
    "We then train the model through multiple epochs, an epoch being a complete pass through all examples in the training and validation sets.\n",
    "\n",
    "At each epoch, if the validation loss is the best we have seen so far, we'll save the parameters of the model and then after training has finished we'll use that model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1895,
     "status": "ok",
     "timestamp": 1708249374643,
     "user": {
      "displayName": "Wenya Wang",
      "userId": "11693014802286625875"
     },
     "user_tz": -480
    },
    "id": "eAjehc9qH1tV",
    "outputId": "fcbc9c1f-258e-4e91-85c8-f98931fd5f84"
   },
   "outputs": [],
   "source": [
    "N_EPOCHS = 10\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut1-model.pt')\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4zxuQwH0H1tV"
   },
   "source": [
    "You may have noticed the loss is not really decreasing and the accuracy is poor. This is due to several issues with the model which we'll improve in the next notebook.\n",
    "\n",
    "Finally, the metric we actually care about, the test loss and accuracy, which we get from our parameters that gave us the best validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 359,
     "status": "ok",
     "timestamp": 1708249387680,
     "user": {
      "displayName": "Wenya Wang",
      "userId": "11693014802286625875"
     },
     "user_tz": -480
    },
    "id": "49vGHz2WH1tV",
    "outputId": "6c315adb-780c-463d-a39c-78432f781544"
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('tut1-model.pt'))\n",
    "\n",
    "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fIO3ZeBgH1tV"
   },
   "source": [
    "## Next Steps\n",
    "\n",
    "In the next notebook, you will work on improving the model to give better performances according to the Assignment instructions (including but not limited to):\n",
    "- Configuration Optimization\n",
    "  \n",
    "  - different hyperparameters, e.g., optimizer, learning rate, batch size, size of the hidden vector\n",
    "  - regularization\n",
    "  - packed padded sequences\n",
    "\n",
    "- Input Embedding\n",
    "\n",
    "  - pre-trained word embeddings\n",
    "\n",
    "- Output Embedding\n",
    "\n",
    "  - sentence embedding computation\n",
    "\n",
    "- Architecture Optimization\n",
    "\n",
    "  - bidirectional RNN\n",
    "  - multi-layer RNN\n",
    "  - LSTM or GRU\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
